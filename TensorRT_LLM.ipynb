{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzx7CJmDNUWvqbnSt3H7DM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartinezai/Llama/blob/main/TensorRT_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY9T_9R-gDS4",
        "outputId": "4dc59fe2-60c0-468c-c224-7887cb9f7a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7J_0c0egJ9Q",
        "outputId": "75915aa0-a108-414c-9704-e93c8faa8701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.11/dist-packages (10.8.0.43)\n",
            "Requirement already satisfied: tensorrt-cu12==10.8.0.43 in /usr/local/lib/python3.11/dist-packages (from tensorrt) (10.8.0.43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python3 -m pip install tensorrt-cu11 tensorrt-lean-cu11 tensorrt-dispatch-cu11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpT6eBQkgPuL",
        "outputId": "b9c2b236-b08d-4767-e193-9619c20f664a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt-cu11 in /usr/local/lib/python3.11/dist-packages (10.8.0.43)\n",
            "Requirement already satisfied: tensorrt-lean-cu11 in /usr/local/lib/python3.11/dist-packages (10.8.0.43)\n",
            "Requirement already satisfied: tensorrt-dispatch-cu11 in /usr/local/lib/python3.11/dist-packages (10.8.0.43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python3 -m pip install --upgrade tensorrt-lean\n",
        "#!python3 -m pip install --upgrade tensorrt-dispatch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sysBsmvShS6j",
        "outputId": "d51d6822-e653-493f-93bf-0c0b58e654cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt-lean in /usr/local/lib/python3.11/dist-packages (10.8.0.43)\n",
            "Requirement already satisfied: tensorrt-lean-cu12==10.8.0.43 in /usr/local/lib/python3.11/dist-packages (from tensorrt-lean) (10.8.0.43)\n",
            "Requirement already satisfied: tensorrt-dispatch in /usr/local/lib/python3.11/dist-packages (10.8.0.43)\n",
            "Requirement already satisfied: tensorrt-dispatch-cu12==10.8.0.43 in /usr/local/lib/python3.11/dist-packages (from tensorrt-dispatch) (10.8.0.43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt\n",
        "print(tensorrt.__version__)\n",
        "assert tensorrt.Builder(tensorrt.Logger())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I46bZgLehq7O",
        "outputId": "fcfa7b2e-ab89-47c5-bbb4-9bddbbfe176a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.8.0.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt_lean as trt\n",
        "print(trt.__version__)\n",
        "assert trt.Runtime(trt.Logger())\n",
        "\n",
        "import tensorrt_dispatch as trt\n",
        "print(trt.__version__)\n",
        "assert trt.Runtime(trt.Logger())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wsAsWX0h66u",
        "outputId": "7a7ef44b-1fba-401f-bd77-4c8b0cae77d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.8.0.43\n",
            "10.8.0.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLnQ1dMmrocQ",
        "outputId": "90fecf84-6650-499e-b169-0a145393e0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting tensorrt_llm\n",
            "  Using cached tensorrt_llm-0.17.0.post1.tar.gz (762 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITL-nXZ_rgqL",
        "outputId": "e7a87494-5bbb-4946-82ae-9d3dc61b0350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!docker run --rm --runtime=nvidia --gpus all --entrypoint /bin/bash -it nvidia/cuda:12.1.0-devel-ubuntu22.04"
      ],
      "metadata": {
        "id": "h-quSqOhsRPM",
        "outputId": "952b688d-de42-4dd8-f455-675915c10edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: docker: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0G13W-TvAthE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorrt_llm --extra-index-url https://pypi.nvidia.com --extra-index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1KajytKlfff",
        "outputId": "de12ec86-1a54-4edb-e1f0-c6ce7cccff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com, https://download.pytorch.org/whl/cu121\n",
            "Collecting tensorrt_llm\n",
            "  Using cached tensorrt_llm-0.17.0.post1.tar.gz (762 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!import huggingface_hub"
      ],
      "metadata": {
        "id": "xkCUmUPGljkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_hub.login(\"\")"
      ],
      "metadata": {
        "id": "zqRbGQ3zllM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_hub.snapshot_download(\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"llama3-hf\")"
      ],
      "metadata": {
        "id": "xiPtVZRim9s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -L https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/llama/convert_checkpoint.py"
      ],
      "metadata": {
        "id": "Uq0l-OBCpV9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_checkpoint.py --model_dir llama3-hf \\\n",
        "    --output_dir ./llama3-safetensors \\\n",
        "    --dtype bfloat16"
      ],
      "metadata": {
        "id": "OmMIJ_d1pazR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e39eb57b-2e6a-448c-b0ac-f9c065e486ac"
      },
      "outputs": [],
      "source": [
        "!pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPuXf5P1Zhxy"
      },
      "outputs": [],
      "source": [
        "!git clone -b v0.9.0 https://github.com/NVIDIA/TensorRT-LLM.git\n",
        "!cd TensorRT-LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -y install libopenmpi-dev && pip3 install --upgrade setuptools && pip3 install tensorrt_llm"
      ],
      "metadata": {
        "id": "NPTZul8GebpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm import LLM, SamplingParams\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    prompts = [\n",
        "        \"Hello, my name is\",\n",
        "        \"The president of the United States is\",\n",
        "        \"The capital of France is\",\n",
        "        \"The future of AI is\",\n",
        "    ]\n",
        "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "    llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "    # Print the outputs.\n",
        "    for output in outputs:\n",
        "        prompt = output.prompt\n",
        "        generated_text = output.outputs[0].text\n",
        "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
        "\n",
        "\n",
        "# The entry point of the program need to be protected for spawning processes.\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "GIRKNBVEeHOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -y install libopenmpi-dev && pip3 install --upgrade setuptools && pip3 install tensorrt_llm\n"
      ],
      "metadata": {
        "id": "ktP_q28HZ8vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "id": "4FZ5GOrvadqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B"
      ],
      "metadata": {
        "id": "YSsXcptIazEl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}